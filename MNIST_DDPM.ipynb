{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DDPM+DDIM with MNIST\n",
    "****"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General Presets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "# Numerical Operations\n",
    "import random\n",
    "import numpy as np\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets\n",
    "# my_utilities\n",
    "import my_utilities as my_utils"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-03T21:40:38.285313Z",
     "end_time": "2024-03-03T21:40:39.517865Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\" For Auto-reload Modules\"\"\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-03T19:56:47.903876Z",
     "end_time": "2024-03-03T19:56:47.926857Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configurations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\" Set paths & device \"\"\"\n",
    "\n",
    "data_dir, checkpoints_dir, results_dir = 'data/', 'checkpoints/', 'results/'\n",
    "device = my_utils.register_device()\n",
    "\n",
    "\"\"\" Set model options \"\"\"\n",
    "\n",
    "hp = {\n",
    "\t# ============= data ==============\n",
    "\t'train_batch_size': 64,\n",
    "\t# ============= model ==============\n",
    "\t'opt': 'DDPM',\n",
    "\t'eta': 0,\n",
    "\t'input_size': 32,\n",
    "\t'in_chan': 1,\n",
    "\t'out_chan': 1,\n",
    "\t't_chan': 256,\n",
    "\t'fst_filters': 64,\n",
    "\t'lst_chan': 1024,\n",
    "\t'groups': 32,\n",
    "\t'drop_rate': 0.1,\n",
    "\t'train_T': 600, # DDIM 700\n",
    "\t'sampling_T': 600,  # DDIM 20\n",
    "\t'beta': 'sin',\n",
    "\t# ============= training ==============\n",
    "\t'init_lr': 6e-5,\n",
    "\t'epoch_num': 15,\n",
    "\t'L2': 5e-3,\n",
    "\t# ============= testing =============\n",
    "\t'samples_num': 5\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-03T19:56:52.172467Z",
     "end_time": "2024-03-03T19:56:52.209952Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Create datasets \"\"\"\n",
    "\n",
    "raw_train_set = datasets.MNIST('./data', train=True,\n",
    "                               transform=\n",
    "                               transforms.Compose([\n",
    "\t                               transforms.Resize((hp['input_size'], hp['input_size'])),\n",
    "\t                               transforms.ToTensor(),\n",
    "\t                               transforms.Normalize((0.5,), (0.5,))\n",
    "                               ]),\n",
    "                               download=False)\n",
    "train_batches = DataLoader(raw_train_set, shuffle=True, batch_size=hp['train_batch_size'], pin_memory=True)\n",
    "print(len(raw_train_set))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-03T20:07:41.601333Z",
     "end_time": "2024-03-03T20:07:41.698052Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\"\"\" Export train set as .png \"\"\"\n",
    "\n",
    "\n",
    "save_dir = 'data/MNIST/png'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "n = len(raw_train_set)\n",
    "cnt = 0\n",
    "for images, _ in train_batches:\n",
    "    for j in range(images.size(0)):\n",
    "        if cnt >= n:\n",
    "            break\n",
    "        save_image(images[j], f'{save_dir}/image_{cnt}.png', normalize=True)\n",
    "        cnt += 1\n",
    "    if cnt >= n:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-03T20:13:53.998590Z",
     "end_time": "2024-03-03T20:14:20.794283Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\"\"\" Define weight initialization \"\"\"\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "\tif isinstance(m, nn.Module):\n",
    "\t\tfor name, param in m.named_parameters():\n",
    "\t\t\tif param.requires_grad:\n",
    "\t\t\t\tname = name.replace('.', '_')\n",
    "\t\t\t\tm.register_buffer(f\"ema_{name}\", param.data.clone())\n",
    "\t\t\t\tif not hasattr(m, 'ema_decay'): m.ema_decay = 0.999"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-16T12:27:18.836561Z",
     "end_time": "2024-02-16T12:27:18.836561Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\"\"\" Define ResBlock & AttnBlock \"\"\"\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "\tdef __init__(self, in_chan, out_chan, t_chan, groups=32, drop_rate=0.1):\n",
    "\t\t\"\"\"\n",
    "\t\tResidual block\n",
    "\t\t:param in_chan: number of input channels\n",
    "\t\t:param out_chan: number of output channels\n",
    "\t\t:param t_chan: number of time-tensor's channels\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.block = nn.ModuleDict()\n",
    "\t\tself.in_chan, self.out_chan, self.t_chan = in_chan, out_chan, t_chan\n",
    "\n",
    "\t\tself.F1, self.S1, self.P1 = 3, 1, 1\n",
    "\t\tself.F2, self.S2, self.P2 = 1, 1, 0\n",
    "\t\tself.groups = groups\n",
    "\t\tself.drop_rate = drop_rate\n",
    "\n",
    "\t\tself.set_block()\n",
    "\n",
    "\t# self.apply(weights_init)\n",
    "\n",
    "\tdef forward(self, X, t):\n",
    "\t\tin_X = X\n",
    "\t\tX = self.block['conv1'](X)\n",
    "\t\tt = self.block['adjust_t'](t)[:, :, None, None]  # resize to same dims as X\n",
    "\t\ty = self.block['conv2'](X + t)  # embed\n",
    "\t\ty += self.block['adjust_x'](in_X)  # shortcut\n",
    "\t\treturn y\n",
    "\n",
    "\tdef set_block(self):\n",
    "\t\t\"\"\"\"\"\"\n",
    "\t\t\"\"\" conv input X \"\"\"\n",
    "\t\tself.block['conv1'] = nn.Sequential(\n",
    "\t\t\tnn.GroupNorm(self.groups, self.in_chan),\n",
    "\t\t\tnn.SiLU(),\n",
    "\t\t\tnn.Conv2d(self.in_chan, self.out_chan, self.F1, self.S1, self.P1))\n",
    "\n",
    "\t\t\"\"\" adjust t \"\"\"\n",
    "\t\tself.block['adjust_t'] = nn.Sequential(\n",
    "\t\t\tnn.SiLU(),\n",
    "\t\t\tnn.Linear(self.t_chan, self.out_chan))\n",
    "\n",
    "\t\t\"\"\" conv (X + t) \"\"\"\n",
    "\t\tself.block['conv2'] = nn.Sequential(\n",
    "\t\t\tnn.GroupNorm(self.groups, self.out_chan),\n",
    "\t\t\tnn.SiLU(),\n",
    "\t\t\tnn.Dropout(self.drop_rate),\n",
    "\t\t\tnn.Conv2d(self.out_chan, self.out_chan, self.F1, self.S1, self.P1))\n",
    "\n",
    "\t\t\"\"\" prepare X for shortcut \"\"\"\n",
    "\t\tself.block['adjust_x'] = nn.Conv2d(self.in_chan, self.out_chan, self.F2, self.S2, self.P2) \\\n",
    "\t\t\tif self.in_chan != self.out_chan else nn.Identity()\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "\tdef __init__(self, chan, heads=1, d_k=None, groups=32):\n",
    "\t\t\"\"\"\n",
    "\t\tSelf-attention Block\n",
    "\t\t:param chan: number of input channels\n",
    "\t\t:param heads: number of attention heads\n",
    "\t\t:param d_k: number of each head's dims\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.chan, self.heads = chan, heads\n",
    "\t\tself.d_k = self.chan if d_k is None else d_k\n",
    "\t\tself.groups = groups\n",
    "\n",
    "\t\tself.prj = nn.Linear(self.chan, self.heads * self.d_k * 3)\n",
    "\t\tself.softmax = nn.Softmax(dim=2)\n",
    "\t\tself.mlp = nn.Linear(self.heads * self.d_k, self.chan)\n",
    "\n",
    "\t# self.apply(weights_init)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# get q, k, v\n",
    "\t\tb, ch, h, w = X.shape\n",
    "\t\tX = X.view(b, ch, -1).permute(0, 2, 1)  # (b, ch, h, w) => (b, h*w, ch) = (b, seq_len, embed_ch)\n",
    "\t\tq, k, v = torch.chunk(self.prj(X).view(b, -1, self.heads, self.d_k * 3), 3, dim=-1)\n",
    "\t\t# self-attention\n",
    "\t\tattn = self.softmax(torch.einsum('bihd,bjhd->bijh', q, k) * (self.d_k ** -0.5))\n",
    "\t\tscores = torch.einsum('bijh,bjhd->bihd', attn, v).view(b, -1, self.heads * self.d_k)\n",
    "\t\ty = self.mlp(scores)\n",
    "\t\t# shortcut\n",
    "\t\ty = (y + X).permute(0, 2, 1).view(b, ch, h, w)\n",
    "\t\treturn y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-16T12:27:18.853142Z",
     "end_time": "2024-02-16T12:27:18.853142Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\"\"\" Define Unet architecture \"\"\"\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "\tdef __init__(self, img_size=32, in_chan=3, out_chan=3, t_chan=256, fst_filters=64, lst_chan=1024, groups=32,\n",
    "\t             drop_rate=0.1, verbose=False):\n",
    "\t\t\"\"\"\n",
    "\t\tComplete Architecture\n",
    "\t\t:param img_size: width (or height) of input img.\n",
    "\t\t:param in_chan: number of input channels\n",
    "\t\t:param out_chan: number of output channels\n",
    "\t\t:param t_chan: number of time tensor channels\n",
    "\t\t:param fst_filters: number of filters in the first conv.\n",
    "\t\t:param lst_chan: number of channels of the encoder's output\n",
    "\t\t:param groups: number of groups in GroupNorm\n",
    "\t\t:param drop_rate: dropout's rate\n",
    "\t\t:param verbose: whether to print messages in the process\n",
    "\t\t\"\"\"\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder = nn.ModuleList()\n",
    "\t\tself.bridge = nn.ModuleDict()\n",
    "\t\tself.decoder = nn.ModuleList()\n",
    "\n",
    "\t\tself.N = img_size\n",
    "\t\tself.in_chan, self.out_chan, self.t_chan = in_chan, out_chan, t_chan\n",
    "\n",
    "\t\tself.fst_filter, self.lst_chan = fst_filters, lst_chan\n",
    "\t\tself.norm, self.groups = nn.GroupNorm, groups\n",
    "\t\tself.drop_rate = drop_rate\n",
    "\n",
    "\t\tself.F1, self.S1, self.P1 = 3, 1, 1\n",
    "\t\tself.F2, self.S2, self.P2 = 4, 2, 1\n",
    "\n",
    "\t\tself.ver = verbose\n",
    "\n",
    "\t\tself.layers_num = 4\n",
    "\t\t# except the first & the last, each layer has conv. & 2 blocks: Res & Self-Attention\n",
    "\t\tself.en_results = []  # all 3 parts of results are to be appended\n",
    "\n",
    "\t\tself.pile_encoder()\n",
    "\t\tif self.ver: print('-----------------------------------------------------')\n",
    "\t\tself.set_bridge()\n",
    "\t\tif self.ver: print('-----------------------------------------------------')\n",
    "\t\tself.pile_decoder()\n",
    "\t\tif self.ver: print('=====================================================\\n')\n",
    "\n",
    "\t\tself.apply(weights_init)\n",
    "\n",
    "\tdef forward(self, X, t):\n",
    "\t\tt = self.t_vector2tensor(t)\n",
    "\t\tlatent = self.en_forward(X, t)\n",
    "\t\tlatent = self.bridge_forward(latent, t)\n",
    "\t\ty = self.de_forward(latent, t)\n",
    "\n",
    "\t\tif self.ver: print(\"\\nforwarding done.\\n\"\n",
    "\t\t                   \"=====================================================\\n\")\n",
    "\t\treturn y\n",
    "\n",
    "\tdef t_vector2tensor(self, t: torch.Tensor):\n",
    "\t\tif self.ver: print(\"init -> t's dims = \" + str(t.shape))\n",
    "\t\thalf_dim = self.t_chan // 8\n",
    "\t\temb = math.log(10_000) / (half_dim - 1)\n",
    "\t\temb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "\t\tt = t.unsqueeze(1) * emb.unsqueeze(0)\n",
    "\t\tif self.ver: print(\"encoded -> t's dims = \" + str(t.shape))\n",
    "\t\tt = torch.cat((t.sin(), t.cos()), dim=1)\n",
    "\t\tif self.ver: print(\"cat -> t's dims = \" + str(t.shape))\n",
    "\t\tt = nn.Linear(self.t_chan // 4, self.t_chan).to(t.device)(t)\n",
    "\t\tif self.ver: print(\"linear1 -> t's dims = \" + str(t.shape))\n",
    "\t\tt = nn.SiLU().to(t.device)(t)\n",
    "\t\tif self.ver: print(\"Swish -> t's dims = \" + str(t.shape))\n",
    "\t\tt = nn.Linear(self.t_chan, self.t_chan).to(t.device)(t)\n",
    "\t\tif self.ver: print(\"linear2 -> t's dims = \" + str(t.shape))\n",
    "\n",
    "\t\tif self.ver: print(\"t_vector2tensor done.\")\n",
    "\t\treturn t\n",
    "\n",
    "\tdef pile_encoder(self):  # downwards\n",
    "\t\tfor i in range(self.layers_num):\n",
    "\t\t\t\"\"\" CONV \"\"\"\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tin_chan = self.in_chan\n",
    "\t\t\t\tout_chan = self.fst_filter\n",
    "\t\t\t\tF, S, P = self.F1, self.S1, self.P1\n",
    "\t\t\telse:\n",
    "\t\t\t\tin_chan = self.fst_filter * (2 ** (i - 1))\n",
    "\t\t\t\tout_chan = in_chan\n",
    "\t\t\t\tF, S, P = self.F2, self.S2, self.P2\n",
    "\t\t\tconv = nn.Conv2d(in_chan, out_chan, F, S, P)\n",
    "\t\t\tself.encoder.append(conv)\n",
    "\t\t\tif self.ver: print(\"(conv)\")\n",
    "\n",
    "\t\t\t\"\"\" RES + ATTN block \"\"\"\n",
    "\t\t\tblock = nn.ModuleDict()\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tin_chan = out_chan = self.fst_filter\n",
    "\t\t\telif i == self.layers_num - 1:\n",
    "\t\t\t\tin_chan, out_chan = self.fst_filter * (2 ** (i - 1)), self.lst_chan\n",
    "\t\t\telse:\n",
    "\t\t\t\tin_chan = self.fst_filter * (2 ** (i - 1))\n",
    "\t\t\t\tout_chan = in_chan * 2\n",
    "\t\t\tblock['res'] = ResBlock(in_chan, out_chan, self.t_chan, hp['groups'], hp['drop_rate'])\n",
    "\t\t\tblock['attn'] = AttnBlock(out_chan, groups=hp['groups'])\n",
    "\t\t\tself.encoder.append(block)\n",
    "\t\t\tif self.ver: print(\"(res -> attn)\")\n",
    "\n",
    "\t\t\t\"\"\" RES + ATTN block \"\"\"\n",
    "\t\t\tblock = nn.ModuleDict()\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tin_chan = out_chan = self.fst_filter\n",
    "\t\t\telif i == self.layers_num - 1:\n",
    "\t\t\t\tin_chan = out_chan = self.lst_chan\n",
    "\t\t\telse:\n",
    "\t\t\t\tin_chan = out_chan = self.fst_filter * (2 ** i)\n",
    "\t\t\tblock['res'] = ResBlock(in_chan, out_chan, self.t_chan, hp['groups'], hp['drop_rate'])\n",
    "\t\t\tblock['attn'] = AttnBlock(out_chan, groups=hp['groups'])\n",
    "\t\t\tself.encoder.append(block)\n",
    "\t\t\tif self.ver: print(\"(res -> attn)\")\n",
    "\n",
    "\t\t\tif self.ver: print(\"piling en-layer \" + str(i + 1) + \" done.\")\n",
    "\n",
    "\tdef set_bridge(self):\n",
    "\t\t\"\"\"\"\"\"\n",
    "\t\t\"\"\" RES + ATTN + RES block \"\"\"\n",
    "\t\tin_chan = out_chan = self.lst_chan\n",
    "\t\tself.bridge['res1'] = ResBlock(in_chan, out_chan, self.t_chan, hp['groups'], hp['drop_rate'])\n",
    "\t\tself.bridge['attn'] = AttnBlock(out_chan, groups=hp['groups'])\n",
    "\t\tself.bridge['res2'] = ResBlock(in_chan, out_chan, self.t_chan, hp['groups'], hp['drop_rate'])\n",
    "\t\tif self.ver: print(\"(res -> attn -> res)\")\n",
    "\t\tif self.ver: print(\"setting bridge done.\")\n",
    "\n",
    "\tdef pile_decoder(self):  # upwards\n",
    "\t\tfor i in range(self.layers_num):\n",
    "\t\t\t\"\"\" RES + ATTN block \"\"\"\n",
    "\t\t\tblock = nn.ModuleDict()\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tin_chan, out_chan = self.lst_chan * 2, self.lst_chan\n",
    "\t\t\telse:\n",
    "\t\t\t\tin_chan = self.lst_chan // (2 ** i)\n",
    "\t\t\t\tout_chan = in_chan // 2\n",
    "\t\t\tblock['res'] = ResBlock(in_chan, out_chan, self.t_chan, hp['groups'], hp['drop_rate'])\n",
    "\t\t\tblock['attn'] = AttnBlock(out_chan, groups=hp['groups'])\n",
    "\t\t\tself.decoder.append(block)\n",
    "\t\t\tif self.ver: print(\"(res -> attn)\")\n",
    "\n",
    "\t\t\t\"\"\" RES + ATTN block \"\"\"\n",
    "\t\t\tblock = nn.ModuleDict()\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tin_chan, out_chan = self.lst_chan * 2, self.lst_chan\n",
    "\t\t\telse:\n",
    "\t\t\t\tin_chan = self.lst_chan // (2 ** i)\n",
    "\t\t\t\tout_chan = in_chan // 2\n",
    "\t\t\tblock['res'] = ResBlock(in_chan, out_chan, self.t_chan, hp['groups'], hp['drop_rate'])\n",
    "\t\t\tblock['attn'] = AttnBlock(out_chan, groups=hp['groups'])\n",
    "\t\t\tself.decoder.append(block)\n",
    "\t\t\tif self.ver: print(\"(res -> attn)\")\n",
    "\n",
    "\t\t\t\"\"\" RES + ATTN block \"\"\"\n",
    "\t\t\tblock = nn.ModuleDict()\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tcat_chan = self.fst_filter * (2 ** (self.layers_num - 2))\n",
    "\t\t\t\tin_chan, out_chan = self.lst_chan + cat_chan, cat_chan\n",
    "\t\t\telif i == self.layers_num - 1:\n",
    "\t\t\t\tcat_chan = self.fst_filter\n",
    "\t\t\t\tin_chan, out_chan = cat_chan + cat_chan, cat_chan\n",
    "\t\t\telse:\n",
    "\t\t\t\tcat_chan = self.lst_chan // (2 ** (i + 2))\n",
    "\t\t\t\tin_chan, out_chan = cat_chan * 2 + cat_chan, cat_chan\n",
    "\t\t\tblock['res'] = ResBlock(in_chan, out_chan, self.t_chan, hp['groups'], hp['drop_rate'])\n",
    "\t\t\tblock['attn'] = AttnBlock(out_chan, groups=hp['groups'])\n",
    "\t\t\tself.decoder.append(block)\n",
    "\t\t\tif self.ver: print(\"(res -> attn)\")\n",
    "\n",
    "\t\t\t\"\"\" CONV^T \"\"\"\n",
    "\t\t\tif i == self.layers_num - 1:\n",
    "\t\t\t\tin_chan = self.fst_filter\n",
    "\t\t\t\tout_chan = self.out_chan\n",
    "\t\t\t\tF, S, P = self.F1, self.S1, self.P1\n",
    "\t\t\telse:\n",
    "\t\t\t\tin_chan = self.fst_filter * (2 ** (self.layers_num - i - 2))\n",
    "\t\t\t\tout_chan = in_chan\n",
    "\t\t\t\tF, S, P = self.F2, self.S2, self.P2\n",
    "\t\t\tconvT = nn.ConvTranspose2d(in_chan, out_chan, F, S, P)\n",
    "\t\t\tself.decoder.append(convT)\n",
    "\t\t\tif self.ver: print(\"(convT)\")\n",
    "\n",
    "\t\t\tif self.ver: print(\"piling de-layer \" + str(i + 1) + \" done.\")\n",
    "\n",
    "\tdef en_forward(self, X, t):\n",
    "\t\tif self.ver: print('=====================================================')\n",
    "\t\tfor i, layer in enumerate(self.encoder, start=1):\n",
    "\t\t\tif not isinstance(layer, nn.ModuleDict):\n",
    "\t\t\t\tself.en_results += [layer(X)]\n",
    "\t\t\t\tif self.ver: print(\"conv...\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.en_results += [layer['attn'](layer['res'](X, t))]\n",
    "\t\t\t\tif self.ver: print(\"res... attn...\")\n",
    "\t\t\tX = self.en_results[-1]\n",
    "\t\t\tif self.ver: print(\"X's size = \" + str(X.shape))\n",
    "\t\t\tif self.ver: print(\"forward of en-sublayer \" + str(i) + \" done.\")\n",
    "\t\t\tif self.ver: print('--------------------------------------')\n",
    "\n",
    "\t\tif self.ver: print(\"en-forward done.\")\n",
    "\t\treturn X\n",
    "\n",
    "\tdef bridge_forward(self, X, t):\n",
    "\t\tif self.ver: print('=====================================================')\n",
    "\t\tX = self.bridge['res2'](self.bridge['attn'](self.bridge['res1'](X, t)), t)\n",
    "\t\tif self.ver: print(\"res... attn... res...\")\n",
    "\t\tif self.ver: print(\"X's size = \" + str(X.shape))\n",
    "\t\tif self.ver: print(\"bridge-forward done.\")\n",
    "\t\treturn X\n",
    "\n",
    "\tdef de_forward(self, X, t):  # X is result of bridge\n",
    "\t\tif self.ver: print('=====================================================')\n",
    "\t\tfor i, layer in enumerate(self.decoder, start=1):\n",
    "\t\t\tif not isinstance(layer, nn.ModuleDict):\n",
    "\t\t\t\tX = layer(X)\n",
    "\t\t\t\tif self.ver: print(\"convT...\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tcat_X = torch.concat((X, self.en_results.pop()), dim=1)\n",
    "\t\t\t\tX = layer['attn'](layer['res'](cat_X, t))\n",
    "\t\t\t\tif self.ver: print(\"res... attn...\")\n",
    "\t\t\tif self.ver: print(\"X's size = \" + str(X.shape))\n",
    "\t\t\tif self.ver: print(\"forward of de-sublayer \" + str(i) + \" done.\")\n",
    "\t\t\tif self.ver: print('--------------------------------------')\n",
    "\n",
    "\t\tif self.ver: print(\"de-forward done.\")\n",
    "\t\treturn X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-16T12:27:18.975174Z",
     "end_time": "2024-02-16T12:27:18.975174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# \"\"\" Test Forwarding \"\"\"\n",
    "# batch = next(iter(train_batches))[0].to(device)\n",
    "# t = my_utils.get_t(hp['batch_size'], 100).to(device)\n",
    "# print(batch.shape)\n",
    "# with torch.no_grad(): y = model.forward(batch,t)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-16T12:27:18.975174Z",
     "end_time": "2024-02-16T12:27:18.975174Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\"\"\" Define EMA update (right after model update) \"\"\"\n",
    "\n",
    "\n",
    "def update_ema(m):\n",
    "\twith torch.no_grad():\n",
    "\t\tfor name, param in m.named_parameters():\n",
    "\t\t\tif param.requires_grad:\n",
    "\t\t\t\tema_name = f\"ema_{name.replace('.', '_')}\"\n",
    "\t\t\t\tema_param = getattr(m, ema_name)\n",
    "\t\t\t\tema_param.mul_(model.ema_decay).add_(param.data, alpha=1 - model.ema_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-16T12:27:18.975174Z",
     "end_time": "2024-02-16T12:27:18.975174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Define EMA_params application (before validation) \"\"\"\n",
    "\n",
    "\n",
    "def apply_ema(m):\n",
    "\torg_params = {}\n",
    "\twith torch.no_grad():\n",
    "\t\tfor name, param in m.named_parameters():\n",
    "\t\t\tif param.requires_grad:\n",
    "\t\t\t\torg_param_name = f\"original_{name.replace('.', '_')}\"\n",
    "\t\t\t\torg_params[org_param_name] = param.data.clone()\n",
    "\t\t\t\tema_name = f\"ema_{name.replace('.', '_')}\"\n",
    "\t\t\t\tema_param = getattr(m, ema_name)\n",
    "\t\t\t\tparam.data.copy_(ema_param)\n",
    "\treturn org_params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Define params_restoration \"\"\"\n",
    "\n",
    "\n",
    "def restore_org_params(m, org_params):\n",
    "\twith torch.no_grad():\n",
    "\t\tfor name, param in m.named_parameters():\n",
    "\t\t\tif param.requires_grad:\n",
    "\t\t\t\torg_param_name = f\"original_{name.replace('.', '_')}\"\n",
    "\t\t\t\tif org_param_name in org_params:\n",
    "\t\t\t\t\tparam.data.copy_(org_params[org_param_name])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\"\"\" Diffusion using X0 to get Xt \"\"\"\n",
    "\n",
    "\n",
    "def diffuse(X0, beta, t, device, noise=None):\n",
    "\t\"\"\"\n",
    "\tDiffusion Process from X0 to Xt\n",
    "\t:param X0: ground-truth image\n",
    "\t:param beta: scheduled variance, larger t larger beta\n",
    "\t:param t: time vector\n",
    "\t:param noise: noise input\n",
    "\t:param device: device\n",
    "\t:return: Xt, noisy img. at t\n",
    "\t\"\"\"\n",
    "\t# get mean & var --- q(Xt|X0) = N(Xt; sqrt(alpha_bar_t) * X0, 1-alpha_bar_t)\n",
    "\talpha_bar_t = my_utils.get_alpha_bar_t(beta, t).to(device)\n",
    "\tmean = torch.sqrt(alpha_bar_t) * X0\n",
    "\tvar = 1 - alpha_bar_t\n",
    "\t# diffuse to Xt\n",
    "\tif noise is None: noise = torch.randn_like(X0).to(device)\n",
    "\tXt = mean + torch.sqrt(var) * noise\n",
    "\treturn Xt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-16T12:27:18.975174Z",
     "end_time": "2024-02-16T12:27:18.975174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train_DDPM(predictor: Unet, batch, loss_func, optimizer, device, reg_lambda=None, verbose=False):\n",
    "\t\"\"\"\n",
    "\tTraining process: diffuse -> predict noise with Unet -> get loss\n",
    "\t:param predictor: Unet model for noise predicting\n",
    "\t:param batch: each element is a ground-truth img. => X0\n",
    "\t:param loss_func: loss function (MSE)\n",
    "\t:param optimizer: Adam with default settings\n",
    "\t:param device: device\n",
    "\t:param verbose: whether to print messages in the process\n",
    "\t:param reg_lambda: regularize weight\n",
    "\t:return: noise_loss, the loss between pred_noise & ground-truth diffusion noise\n",
    "\t\"\"\"\n",
    "\t# prepare inputs\n",
    "\tbatch = batch.to(device)\n",
    "\tbatch_size = batch.shape[0]\n",
    "\tt = my_utils.get_t(batch_size, hp['train_T']).to(device)\n",
    "\tbeta = my_utils.variance_schedule(hp['train_T'], hp['beta']).to(device)\n",
    "\tnoise = torch.randn_like(batch).to(device)\n",
    "\t# diffusion\n",
    "\tXt = diffuse(batch, beta, t, device, noise)\n",
    "\t# predict noise\n",
    "\toptimizer.zero_grad()\n",
    "\twith autocast():\n",
    "\t\tpredictor.zero_grad()\n",
    "\t\tpred_noise = predictor(Xt, t)\n",
    "\t\tif verbose: print(\"Predicting diffusion noise done.\")\n",
    "\t\t# get loss\n",
    "\t\tif reg_lambda is not None:\n",
    "\t\t\treg_loss_func = nn.L1Loss()\n",
    "\t\t\treg_loss = reg_lambda * reg_loss_func(pred_noise, noise)\n",
    "\t\telse:\n",
    "\t\t\treg_loss = 0\n",
    "\t\tnoise_loss = loss_func(pred_noise, noise) + reg_loss\n",
    "\t\tif verbose: print(\"Got loss of diffusion noise: \", str(noise_loss.data.item()))\n",
    "\t\tscaler.scale(noise_loss).backward()\n",
    "\t\tscaler.step(optimizer)\n",
    "\t\tscaler.update()\n",
    "\t\tupdate_ema(predictor)\n",
    "\t\toptimizer.zero_grad()\n",
    "\treturn noise_loss.data.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-16T12:27:19.073681Z",
     "end_time": "2024-02-16T12:27:19.074681Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\"\"\" Sampling using trained model (sample after each epoch) \"\"\"\n",
    "\n",
    "\n",
    "#  X1 -> X0\n",
    "def discrete_decoder(X1, beta, device, opt='DDPM'):\n",
    "\t# predict noise_t\n",
    "\tt0 = X1.new_full((X1.shape[0],), 0, dtype=torch.long)\n",
    "\tpred_noise = model.forward(X1, t0)\n",
    "\talpha_0 = my_utils.get_alpha_t(beta, t0).to(device)\n",
    "\talpha_bar_0 = my_utils.get_alpha_bar_t(beta, t0).to(device)\n",
    "\tX0 = None\n",
    "\tif opt == 'DDIM':\n",
    "\t\tpred_X0 = (X1 - torch.sqrt(1 - alpha_bar_0) * pred_noise) / torch.sqrt(alpha_bar_0)  # mean\n",
    "\t\t# alpha_init = 1  # the paper assume the alpha_0 to be 1, here alpha_init (alpha from 0 to T-1 following t)\n",
    "\t\t# so sigma_0 = 0, no noise\n",
    "\t\tX0 = pred_X0\n",
    "\telif opt == \"DDPM\":\n",
    "\t\talpha_bar_0 = my_utils.get_alpha_bar_t(beta, t0)\n",
    "\t\tmean = (X1 - ((1 - alpha_0) * pred_noise) / torch.sqrt(1 - alpha_bar_0)) / torch.sqrt(alpha_0)\n",
    "\t\tvar = 1 - alpha_0\n",
    "\t\teps = 1e-5\n",
    "\t\tdelta_plus = torch.where(\n",
    "\t\t\tX1 >= 1 - eps,\n",
    "\t\t\tfloat('inf'),\n",
    "\t\t\t1 / (X1 + 1 + eps) / 255\n",
    "\t\t)\n",
    "\t\tdelta_minus = torch.where(\n",
    "\t\t\tX1 <= -1 + eps,\n",
    "\t\t\tfloat('-inf'),\n",
    "\t\t\t1 / (1 - X1 + eps) / 255\n",
    "\t\t)\n",
    "\t\tmean = torch.where(\n",
    "\t\t\tX1 >= 1 - eps,\n",
    "\t\t\tdelta_plus,\n",
    "\t\t\ttorch.where(\n",
    "\t\t\t\tX1 <= -1 + eps,\n",
    "\t\t\t\tdelta_minus,\n",
    "\t\t\t\tmean\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t\tprob = torch.distributions.Normal(mean, torch.sqrt(var))  # P(X0|X1)\n",
    "\t\tX0 = prob.sample()\n",
    "\treturn X0\n",
    "\n",
    "\n",
    "#  others\n",
    "def denoise(X, beta, t, nxt_t, device, opt='DDPM', eta=0):\n",
    "\t\"\"\"\n",
    "\tDenoise process from X{t+1} to X{nxt_t+1}\n",
    "\t:param X: noisy img. at t+1\n",
    "\t:param beta: scheduled variance, larger t larger beta\n",
    "\t:param t: time step vector\n",
    "\t:param nxt_t: next t vector\n",
    "\t:param device: device\n",
    "\t:param opt: DDPM or DDIM\n",
    "\t:param eta: interpolation between DDPM & DDIM\n",
    "\t:return: denoised X\n",
    "\t\"\"\"\n",
    "\t# predict noise_t\n",
    "\tpred_noise = model.forward(X, t)\n",
    "\t# get mean & var --- P(X{t-1}|Xt) = N(X{t-1}; mu(Xt, t, pred_noise), beta_t)\n",
    "\talpha_t = my_utils.get_alpha_t(beta, t).to(device)\n",
    "\talpha_bar_t = my_utils.get_alpha_bar_t(beta, t)\n",
    "\tnoise = torch.randn(X.shape, device=device)\n",
    "\tif opt == 'DDIM':\n",
    "\t\tpred_X0 = (X - torch.sqrt(1 - alpha_bar_t) * pred_noise) / torch.sqrt(alpha_bar_t)\n",
    "\t\talpha_bar_nxt_t = my_utils.get_alpha_bar_t(beta, nxt_t)\n",
    "\t\t# beta = 1 - alpha_bar_t / alpha_bar_nxt_t\n",
    "\t\t# var(DDIM) = eta*(1 - alpha_bar_nxt_t) / (1 - alpha_bar_t)*beta\n",
    "\t\tvar = eta * (1 - alpha_bar_nxt_t) / (1 - alpha_bar_t) * \\\n",
    "\t\t          (1 - alpha_bar_t / alpha_bar_nxt_t)\n",
    "\t\tdirct2Xt = torch.sqrt(1 - alpha_bar_nxt_t - var) * pred_noise\n",
    "\t\tX = torch.sqrt(alpha_bar_nxt_t) * pred_X0 + dirct2Xt + torch.sqrt(var) * noise\n",
    "\telif opt == 'DDPM':\n",
    "\t\tmean = (X - ((1 - alpha_t) * pred_noise) / torch.sqrt(1 - alpha_bar_t)) / torch.sqrt(alpha_t)\n",
    "\t\tvar = 1 - alpha_t\n",
    "\t\tX = mean + torch.sqrt(var) * noise\n",
    "\treturn X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-16T12:27:19.074681Z",
     "end_time": "2024-02-16T12:27:19.074681Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# \"\"\" Test training \"\"\"\n",
    "#\n",
    "# batch = next(iter(train_batches))[0]\n",
    "# train_DDPM(model, batch, nn.MSELoss(), optim.Adam(model.parameters(), lr=hp['init_lr']), device, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-16T12:27:19.074681Z",
     "end_time": "2024-02-16T12:27:19.074681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Complete Training Process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\tComplete Training\n",
    "\"\"\"\n",
    "\n",
    "model = Unet(\n",
    "\thp['input_size'],\n",
    "\thp['in_chan'],\n",
    "\thp['out_chan'],\n",
    "\thp['t_chan'],\n",
    "\thp['fst_filters'],\n",
    "\thp['lst_chan'],\n",
    "\thp['groups'],\n",
    "\thp['drop_rate']\n",
    ").to(device)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hp['init_lr'], weight_decay=hp['L2'])\n",
    "\n",
    "# set epoch_num\n",
    "e_num = hp['epoch_num']\n",
    "\n",
    "# train loop\n",
    "e_loss = []  # losses of each epoch\n",
    "min_loss = None  # save the minimum loss (after each epoch)\n",
    "for e in range(e_num):  # iter. epochs\n",
    "\t\"\"\" training \"\"\"\n",
    "\tmodel.train()\n",
    "\tb_loss, b, tol_loss = [], 0, 0  # b_loss: losses (average) of each batch\n",
    "\tfor batch in train_batches:  # iter. batches\n",
    "\t\tb += 1\n",
    "\t\tloss = train_DDPM(model, batch[0], loss_func, optimizer, device)\n",
    "\t\tb_loss.append(loss)\n",
    "\t\ttol_loss += loss\n",
    "\t\tif (b - 1) % 100 == 0: print(\"epoch \", e + 1, \"/\", e_num, \"train_batch \", b, \" => loss: \", loss)\n",
    "\tbatch_num = b\n",
    "\tavg_loss = tol_loss / batch_num\n",
    "\tprint(\"epoch \", e + 1, \"/\", e_num, \" => avg. train_loss: \", avg_loss)\n",
    "\torg_params = apply_ema(model)  # save original params\n",
    "\tif min_loss is None or avg_loss < min_loss:\n",
    "\t\tmin_loss = avg_loss\n",
    "\t\tmy_utils.save_model_chk_point(\n",
    "\t\t\tcheckpoints_dir,\n",
    "\t\t\te, model, min_loss,\n",
    "\t\t\toptimizer\n",
    "\t\t)\n",
    "\n",
    "\t\"\"\" sampling \"\"\"\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tXt = torch.randn([hp['samples_num'], hp['in_chan'], hp['input_size'], hp['input_size']], device=device)\n",
    "\t\tbeta = my_utils.variance_schedule(hp['train_T'], hp['beta']).to(device)\n",
    "\t\tc = hp['train_T'] // hp['sampling_T']\n",
    "\t\tt_seq = list(range(0, hp['train_T'], c))\n",
    "\t\tfor i in range(len(t_seq)-1, -1, -1):  # t from T-1 to 0\n",
    "\t\t\tif i != 0:\n",
    "\t\t\t\tt, nxt_t = t_seq[i], t_seq[i-1]\n",
    "\t\t\t\tt = Xt.new_full((hp['samples_num'],), t, dtype=torch.long)\n",
    "\t\t\t\tnxt_t = Xt.new_full((hp['samples_num'],), nxt_t, dtype=torch.long)\n",
    "\t\t\t\tXt = denoise(Xt, beta, t, nxt_t, device, hp['opt'], hp['eta'])\n",
    "\t\t\telse:\n",
    "\t\t\t\tX0 = discrete_decoder(Xt, beta, device, hp['opt'])\n",
    "\t\tmy_utils.save_gen_chk_point(X0, results_dir, e + 1)\n",
    "\n",
    "\t\"\"\" restore org_params \"\"\"\n",
    "\trestore_org_params(model, org_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-24T18:10:08.477465Z",
     "end_time": "2024-02-24T18:10:08.477970Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: conda: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# ! python -m pytorch_fid data/MNIST/png results\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-03T21:42:43.661217Z",
     "end_time": "2024-03-03T21:42:43.821217Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
